{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORT NECESSARY LIBRARIES AND DATASET**"
      ],
      "metadata": {
        "id": "tJSFHJvqmro3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJCC830Umbx5"
      },
      "outputs": [],
      "source": [
        "#Importing necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Uploading data\n",
        "\n",
        "data=pd.read_csv(\"/content/drive/MyDrive/Orders.csv\", header = 0, encoding='utf-8')"
      ],
      "metadata": {
        "id": "G3RRac9YmzhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA PREPROCESSING & EXPLORATORY DATA ANALYSIS**"
      ],
      "metadata": {
        "id": "1jPM44A-m91w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data I used belongs to an organisation therefore, I cannot share all the data preprocessing and EDA cells, but I will list down the steps."
      ],
      "metadata": {
        "id": "Jxk-1eHUryx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In this stage the uploaded data needs to be preprocessed by checking for incorrect datatypes and null values. Null values can either be filled by imputation or they can be removed entirely depending upon the feature for which they exist and their importance."
      ],
      "metadata": {
        "id": "HGHiDWROnBZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Feature construction needs to be performed if more features can be derived from the existing information in the dataset. Since my data was on order product level and for customer segmentation it had to be on a customer level, therefore I group my data on basis of unique customers. While doing so I was able to construct new features such as Order Count, Average Order Value etc."
      ],
      "metadata": {
        "id": "oZzi8oh-nv3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Using boxplots and histogram the spread of the data needs to be checked, and outliers identified. Next, the data needs to be normalised so that all values are on the same scale and accurately processed."
      ],
      "metadata": {
        "id": "gcs0JCHIoqtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Next, correlation needs to be checked in between all the features:"
      ],
      "metadata": {
        "id": "UYfWHSsysvNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Computing the correlation matrix for dataset\n",
        "\n",
        "corr_matrix = clean_data[numeric_cols].corr()\n",
        "\n",
        "#Plotting the heatmap\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"magma\", cbar=True, annot_kws={\"size\": 6})\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "#Printing the correlation values rounded to two decimal places\n",
        "\n",
        "print(corr_matrix.round(2))"
      ],
      "metadata": {
        "id": "f6IyStYTm6Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PRINCIPAL COMPONENT ANALYSIS (PCA)**"
      ],
      "metadata": {
        "id": "HeNRsptVteOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA is used to reduce the dimensionality to counter the curse of dimensionality. This helps the model from becoming too complex and slow. I have named my dataset clean_data."
      ],
      "metadata": {
        "id": "SI9ITwZXtr7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining numeric cols\n",
        "\n",
        "exclude_cols = ['UserID']\n",
        "numeric_cols = clean_data.select_dtypes(include=['int64', 'float64','bool']).columns.difference(exclude_cols).tolist()"
      ],
      "metadata": {
        "id": "a4k9WePQtho9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the necessary library\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Defining the data\n",
        "X_b2b = clean_data[numeric_cols].copy()\n",
        "\n",
        "#Initialising PCA\n",
        "\n",
        "pca = PCA(n_components=20, random_state=42)\n",
        "X_pca = pca.fit_transform(X_b2b)\n",
        "\n",
        "#Explained variance\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained variance ratio by PCA components:\", explained_variance)\n",
        "\n",
        "#Plotting cumulative explained variance\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1, len(explained_variance)+1), explained_variance.cumsum(), marker='o')\n",
        "plt.xlabel('Number of PCA Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "#PCA for 2D visualisation\n",
        "\n",
        "pca_2d = PCA(n_components=2, random_state=42)\n",
        "X_pca_2d = pca_2d.fit_transform(X_b2b)\n",
        "\n",
        "#Converting to a dataFrame for easy plotting\n",
        "\n",
        "df_pca_2d_b2b = pd.DataFrame(X_pca_2d, columns=['PCA1', 'PCA2'])\n",
        "\n",
        "# Plot 2D scatter plot\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x='PCA1', y='PCA2', data=df_pca_2d_b2b, c = 'blue', edgecolor = 'k', s = 50)\n",
        "plt.title('PCA 2D Visualization of Features B2B Dataset')\n",
        "plt.xlabel('PCA Component 1')\n",
        "plt.ylabel('PCA Component 2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y6tZgqbMudau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ELBOW METHOD**"
      ],
      "metadata": {
        "id": "KYcCKkF2u9xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elbow Method is used to identify the optimal number of clusters. First for 100 clusters, then for 50 for clear elbow indentification."
      ],
      "metadata": {
        "id": "b9wnslVkvCzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary library\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#Using PCA-reduced data for clustering\n",
        "\n",
        "X_cluster = X_pca\n",
        "\n",
        "#Testing different values of k\n",
        "\n",
        "inertia = []\n",
        "K = range(1, 100)\n",
        "\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_cluster)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "#Plotting the Elbow curve\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(K, inertia, 'bo-', markersize=8)\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Inertia (Sum of squared distances)\")\n",
        "plt.title(\"Elbow Method to Determine Optimal k for Dataset\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A3l2qGYSu_oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing different values of k\n",
        "\n",
        "inertia = []\n",
        "K = range(1, 50)\n",
        "\n",
        "for k in K:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X_cluster)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "#Plotting the Elbow curve\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(K, inertia, 'bo-', markersize=8)\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Inertia (Sum of squared distances)\")\n",
        "plt.title(\"Elbow Method to Determine Optimal k for Dataset\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P1z1IDvQvhu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **APPLYING TRADITIONAL MACHINE LEARNING CLUSTERING ALGORITHMS**"
      ],
      "metadata": {
        "id": "I8_d0qZvvpgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **KMEANS**"
      ],
      "metadata": {
        "id": "sqn7kVYWvwaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search is used to evaluate multiple hyperparameter combinations."
      ],
      "metadata": {
        "id": "NGHgnq8Uv130"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "#Defining the directory to save CSVs\n",
        "save_dir = '/content/drive/MyDrive/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#X_cluster is the pca reduced data\n",
        "X_cluster = np.asarray(X_pca, dtype=np.float64)\n",
        "\n",
        "#Defining the parameter grid\n",
        "cluster_range = [2, 3, 4, 5, 6, 7]\n",
        "init_methods = ['k-means++', 'random']\n",
        "n_init_values = [10, 20, 30, 40]\n",
        "\n",
        "#Storing results\n",
        "results = []\n",
        "\n",
        "#Grid search\n",
        "for n_clusters in cluster_range:\n",
        "    for init in init_methods:\n",
        "        for n_init in n_init_values:\n",
        "            #Initialising KMeans\n",
        "            kmeans = KMeans(\n",
        "                n_clusters=n_clusters,\n",
        "                init=init,\n",
        "                n_init=n_init,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            #Fitting KMeans\n",
        "            labels = kmeans.fit_predict(X_cluster)\n",
        "\n",
        "            #Computing metrics\n",
        "            sil = silhouette_score(X_cluster, labels)\n",
        "            dbi = davies_bouldin_score(X_cluster, labels)\n",
        "\n",
        "            #Printing results\n",
        "            print(f\"Clusters = {n_clusters}, init = {init}, n_init = {n_init}, Silhouette Score = {sil:.4f}, DB Index = {dbi:.4f}\")\n",
        "\n",
        "            #Storing in results\n",
        "            results.append({\n",
        "                'Clusters': n_clusters,\n",
        "                'Init': init,\n",
        "                'n_init': n_init,\n",
        "                'Silhouette': sil,\n",
        "                'DBI': dbi\n",
        "            })\n",
        "\n",
        "            #Storing the cluster labels for each combination\n",
        "            label_df = clean_data.copy()\n",
        "            label_df['Cluster'] = labels\n",
        "\n",
        "            filename = os.path.join(save_dir,f'KMeans_PCA_n{n_clusters}_init_{init}_ninit_{n_init}.csv')\n",
        "            label_df.to_csv(filename, index=False)\n",
        "            print(f\"Saved cluster labels: {filename}\")\n",
        "\n",
        "#Converting results to DataFrame for metrics overview\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "#Saving the grid search metrics results to CSV\n",
        "metrics_path = os.path.join(save_dir, 'KMeans_PCA_grid_search_metrics.csv')\n",
        "results_df.to_csv(metrics_path, index=False)\n",
        "print(f\"\\nGrid search metrics saved to: {metrics_path}\")"
      ],
      "metadata": {
        "id": "8qhleI3WvvPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GAUSSIAN MIXTURE MODELS (GMM)**"
      ],
      "metadata": {
        "id": "yLK1XatbwNSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search is used to evaluate multiple hyperparameter combinations."
      ],
      "metadata": {
        "id": "p4yNYkoFwfZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "#Defining directory to save CSVs\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#Preparing and defining standardised dataset\n",
        "\n",
        "X_cluster = X_pca\n",
        "\n",
        "#Defining grid search parameters\n",
        "\n",
        "cluster_range = [2, 3, 4, 5, 6, 7]\n",
        "covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
        "\n",
        "results = []\n",
        "\n",
        "#Grid Search\n",
        "\n",
        "for n_clusters in cluster_range:\n",
        "    for cov_type in covariance_types:\n",
        "        print(f\"Fitting GMM: n_clusters={n_clusters}, covariance_type={cov_type}\")\n",
        "\n",
        "        try:\n",
        "            #Initialising and fitting GMM\n",
        "            gmm = GaussianMixture(n_components=n_clusters, covariance_type=cov_type, random_state=42)\n",
        "            labels = gmm.fit_predict(X_cluster)\n",
        "\n",
        "            #Only compute metrics if there is more than 1 cluster\n",
        "            if len(np.unique(labels)) < 2:\n",
        "                sil, dbi = np.nan, np.nan\n",
        "                print(f\"Skipped: Only one cluster found for n_clusters={n_clusters}, cov_type={cov_type}\")\n",
        "            else:\n",
        "                sil = silhouette_score(X_cluster, labels)\n",
        "                dbi = davies_bouldin_score(X_cluster, labels)\n",
        "                print(f\"n_clusters={n_clusters}, cov_type={cov_type}, Silhouette={sil:.4f}, DBI={dbi:.4f}\")\n",
        "\n",
        "            #Saving labels for every combination\n",
        "            combo_labels = clean_data.copy()\n",
        "            combo_labels['Cluster'] = labels\n",
        "            out_path = os.path.join(save_dir,f'GMM_Clusters_{n_clusters}_Cov_{cov_type}.csv')\n",
        "            combo_labels.to_csv(out_path, index=False)\n",
        "            print(f\"Cluster labels saved to {out_path}\")\n",
        "\n",
        "\n",
        "            #Storing results\n",
        "            results.append({\n",
        "                'n_clusters': n_clusters,\n",
        "                'covariance_type': cov_type,\n",
        "                'Silhouette': sil,\n",
        "                'DBI': dbi\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error for n_clusters={n_clusters}, cov_type={cov_type}: {e}\")\n",
        "            results.append({'n_clusters': n_clusters, 'covariance_type': cov_type, 'Silhouette': np.nan, 'DBI': np.nan})\n",
        "\n",
        "#Converting results to dataFrame and exporting them\n",
        "results_df = pd.DataFrame(results)\n",
        "metrics_file = os.path.join(save_dir, 'GMM_grid_search_results.csv')\n",
        "results_df.to_csv(metrics_file, index=False)\n",
        "print(f\"\\nGrid search metrics saved to '{metrics_file}'\")"
      ],
      "metadata": {
        "id": "bM7-JcEfwYeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BIRCH**"
      ],
      "metadata": {
        "id": "hIX7u1AYwh6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search is used to evaluate multiple hyperparameter combinations."
      ],
      "metadata": {
        "id": "tir-5C_OwkV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import Birch\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "import os\n",
        "\n",
        "#Defining directory to save CSVs\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#Preparing and defining pca reduced dataset\n",
        "\n",
        "X_cluster = X_pca\n",
        "\n",
        "#Defining grid search parameters\n",
        "\n",
        "cluster_range = [2, 3, 4, 5, 6, 7]\n",
        "thresholds = [0.2, 0.3, 0.5, 0.6]\n",
        "\n",
        "results = []\n",
        "\n",
        "#Grid Search\n",
        "\n",
        "for n_clusters in cluster_range:\n",
        "    for thresh in thresholds:\n",
        "        print(f\"Fitting BIRCH: n_clusters={n_clusters}, threshold={thresh}\")\n",
        "\n",
        "        try:\n",
        "            #Initialising and fitting BIRCH\n",
        "            birch = Birch(n_clusters=n_clusters, threshold=thresh)\n",
        "            labels = birch.fit_predict(X_cluster)\n",
        "\n",
        "            #Only compute metrics if more than 1 cluster\n",
        "            if len(np.unique(labels)) < 2:\n",
        "                sil, dbi = np.nan, np.nan\n",
        "                print(f\"Skipped: Only one cluster found for n_clusters={n_clusters}, threshold={thresh}\")\n",
        "            else:\n",
        "                sil = silhouette_score(X_cluster, labels)\n",
        "                dbi = davies_bouldin_score(X_cluster, labels)\n",
        "                print(f\"n_clusters={n_clusters}, threshold={thresh}, Silhouette={sil:.4f}, DBI={dbi:.4f}\")\n",
        "\n",
        "            #Saving clusters for every combo\n",
        "            labels_df = clean_data.copy()\n",
        "            labels_df['Cluster'] = labels\n",
        "            out_file = os.path.join(save_dir, f'BIRCH_Clusters_{n_clusters}_Threshold_{thresh}.csv')\n",
        "            labels_df.to_csv(out_file, index=False)\n",
        "            print(f\"Cluster labels saved to {out_file}\")\n",
        "\n",
        "            #Storing results\n",
        "            results.append({\n",
        "                'n_clusters': n_clusters,\n",
        "                'threshold': thresh,\n",
        "                'Silhouette': sil,\n",
        "                'DBI': dbi\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error for n_clusters={n_clusters}, threshold={thresh}: {e}\")\n",
        "            results.append({\n",
        "                'n_clusters': n_clusters,\n",
        "                'threshold': thresh,\n",
        "                'Silhouette': np.nan,\n",
        "                'DBI': np.nan\n",
        "            })\n",
        "\n",
        "#Converting results to dataframe and exporting them\n",
        "results_df = pd.DataFrame(results)\n",
        "metrics_file = os.path.join(save_dir, 'BIRCH_grid_search_results.csv')\n",
        "results_df.to_csv(metrics_file, index=False)\n",
        "print(f\"\\nGrid search metrics saved to '{metrics_file}'\")"
      ],
      "metadata": {
        "id": "MPQFZQvNwj3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **APPLYING DEEP LEARNING CLUSTERING ALGORITHMS**"
      ],
      "metadata": {
        "id": "LYXvDsPOwlp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AUTOENCODER + KMEANS**"
      ],
      "metadata": {
        "id": "nlXk1igxwyHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search is used to evaluate multiple hyperparameter combinations."
      ],
      "metadata": {
        "id": "fJj5yi5Cw1Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#Defining directory to save CSVs\n",
        "save_dir = '/content/drive/MyDrive/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#X_cluster: PCA-reduced features for clustering\n",
        "X_cluster = X_pca\n",
        "\n",
        "#Function to build an Autoencoder\n",
        "def build_autoencoder(input_dim, latent_dim):\n",
        "    #Encoder\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(128, activation='relu')(input_layer)\n",
        "    encoded = Dense(64, activation='relu')(encoded)\n",
        "    latent = Dense(latent_dim, activation='relu')(encoded)\n",
        "\n",
        "    #Decoder\n",
        "    decoded = Dense(64, activation='relu')(latent)\n",
        "    decoded = Dense(128, activation='relu')(decoded)\n",
        "    output_layer = Dense(input_dim, activation='linear')(decoded)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    encoder = Model(inputs=input_layer, outputs=latent)\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    return autoencoder, encoder\n",
        "\n",
        "#Grid search parameters\n",
        "latent_dims = [5, 10, 15]\n",
        "cluster_range = [2, 3, 4, 5, 6, 7]\n",
        "learning_rates = [0.001, 0.005, 0.01]\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "dec_iterations = 50\n",
        "\n",
        "results = []\n",
        "\n",
        "for latent_dim in latent_dims:\n",
        "    for n_clusters in cluster_range:\n",
        "        for lr in learning_rates:\n",
        "            print(f\"Training autoencoder: latent_dim={latent_dim}, n_clusters={n_clusters}, lr={lr}\")\n",
        "\n",
        "            autoencoder, encoder = build_autoencoder(input_dim=X_cluster.shape[1], latent_dim=latent_dim)\n",
        "            autoencoder.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
        "\n",
        "            #Training autoencoder\n",
        "            autoencoder.fit(X_cluster, X_cluster, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "            #Getting latent features\n",
        "            latent_features = encoder.predict(X_cluster)\n",
        "\n",
        "            #Applying K-Means in latent space\n",
        "            kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, random_state=42)\n",
        "            labels = kmeans.fit_predict(latent_features)\n",
        "\n",
        "            #Evaluating clustering\n",
        "            #Only calculate metrics if more than 1 cluster\n",
        "            if len(np.unique(labels)) < 2:\n",
        "                sil, dbi = np.nan, np.nan\n",
        "                print(f\"Skipped: Only one cluster found for latent_dim={latent_dim}, n_clusters={n_clusters}, lr={lr}\")\n",
        "            else:\n",
        "                sil = silhouette_score(latent_features, labels)\n",
        "                dbi = davies_bouldin_score(latent_features, labels)\n",
        "                print(f\"Clusters={n_clusters}, Latent={latent_dim}, LR={lr}, Silhouette={sil:.4f}, DBI={dbi:.4f}\")\n",
        "\n",
        "            #Storing results\n",
        "            results.append({'n_clusters': n_clusters, 'latent_dim': latent_dim, 'learning_rate': lr, 'Silhouette': sil, 'DBI': dbi})\n",
        "\n",
        "            #Saving cluster labels per combination\n",
        "            labels_df = clean_data.copy()\n",
        "            labels_df['Cluster'] = labels\n",
        "            filename = os.path.join(save_dir, f'Autoencoder_KMeans_Clusters_{n_clusters}_Latent_{latent_dim}_LR_{lr}.csv')\n",
        "            labels_df.to_csv(filename, index=False)\n",
        "            print(f\"Cluster labels saved to {filename}\")\n",
        "\n",
        "#Converting results to DataFrame for easy viewing\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "#Saving the grid search metrics results to CSV\n",
        "results_metrics_file = os.path.join(save_dir, 'Autoencoder_KMeans_grid_search_metrics.csv')\n",
        "results_df.to_csv(results_metrics_file, index=False)\n",
        "print(f\"\\nGrid search metrics saved to '{results_metrics_file}'\")"
      ],
      "metadata": {
        "id": "CTlmRqCDwxP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DEEP EMBEDDED CLUSTERING (DEC)**"
      ],
      "metadata": {
        "id": "GmUkbI6ww2UB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid search is used to evaluate multiple hyperparameter combinations."
      ],
      "metadata": {
        "id": "9vpSdoV6w6eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "#Defining directory to save CSVs\n",
        "\n",
        "save_dir = '/content/drive/MyDrive/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#Autoencoder builder function\n",
        "\n",
        "def build_autoencoder(input_dim, latent_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoded = Dense(128, activation='relu')(input_layer)\n",
        "    encoded = Dense(64, activation='relu')(encoded)\n",
        "    latent = Dense(latent_dim, activation='relu')(encoded)\n",
        "\n",
        "    decoded = Dense(64, activation='relu')(latent)\n",
        "    decoded = Dense(128, activation='relu')(decoded)\n",
        "    output_layer = Dense(input_dim, activation='linear')(decoded)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    encoder = Model(inputs=input_layer, outputs=latent)\n",
        "\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "    return autoencoder, encoder\n",
        "\n",
        "#Grid Search Parameters\n",
        "latent_dims = [5, 10, 15]\n",
        "cluster_range = [2, 3, 4, 5, 6, 7]\n",
        "learning_rates = [0.001, 0.01]\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "dec_iterations = 50\n",
        "\n",
        "#Preparing dataset (Ensure all numeric values, no NaNs)\n",
        "X_cluster = X_pca\n",
        "\n",
        "#Grid Search\n",
        "\n",
        "results = []\n",
        "\n",
        "for latent_dim in latent_dims:\n",
        "    for n_clusters in cluster_range:\n",
        "        for lr in learning_rates:\n",
        "            print(f\"Training Autoencoder: latent_dim={latent_dim}, n_clusters={n_clusters}, lr={lr}\")\n",
        "\n",
        "            #Building and training autoencoder\n",
        "            autoencoder, encoder = build_autoencoder(input_dim=X_cluster.shape[1], latent_dim=latent_dim)\n",
        "            autoencoder.compile(optimizer=Adam(learning_rate=lr), loss='mse')\n",
        "            autoencoder.fit(X_cluster, X_cluster, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "\n",
        "            #Extracting latent features\n",
        "            latent_features = encoder.predict(X_cluster)\n",
        "\n",
        "            #Initialising K-Means in latent space\n",
        "            kmeans = KMeans(n_clusters=n_clusters, n_init=20, random_state=42)\n",
        "            labels = kmeans.fit_predict(latent_features)\n",
        "            centroids = kmeans.cluster_centers_\n",
        "\n",
        "            #DEC iterative refinement\n",
        "            for iteration in range(dec_iterations):\n",
        "                # Soft assignments (Student's t-distribution)\n",
        "                q = 1.0 / (1.0 + np.sum((latent_features[:, None, :] - centroids[None, :, :])**2, axis=2))\n",
        "                q = q / q.sum(axis=1, keepdims=True)\n",
        "\n",
        "                #Target distribution\n",
        "                p = (q**2) / q.sum(axis=0)\n",
        "                p = p / p.sum(axis=1, keepdims=True)\n",
        "\n",
        "                #Updating centroids\n",
        "                for i in range(n_clusters):\n",
        "                    weights = p[:, i][:, None]\n",
        "                    centroids[i] = np.sum(weights * latent_features, axis=0) / np.sum(weights)\n",
        "\n",
        "            #Final hard labels\n",
        "            final_labels = q.argmax(axis=1)\n",
        "\n",
        "            #Metrics (guard for single cluster)\n",
        "            if len(np.unique(final_labels)) < 2:\n",
        "                sil, dbi = np.nan, np.nan\n",
        "                print(f\"Skipped metrics: only one cluster for latent_dim={latent_dim}, n_clusters={n_clusters}, lr={lr}\")\n",
        "            else:\n",
        "                sil = silhouette_score(latent_features, final_labels)\n",
        "                dbi = davies_bouldin_score(latent_features, final_labels)\n",
        "                print(f\"Clusters={n_clusters}, Latent={latent_dim}, LR={lr}, Silhouette={sil:.4f}, DBI={dbi:.4f}\")\n",
        "\n",
        "            #Saving labels for this combo\n",
        "            labels_df = clean_data.copy()\n",
        "            labels_df['Cluster'] = final_labels\n",
        "            out_file = os.path.join(save_dir,f'DEC_Clusters_{n_clusters}_Latent_{latent_dim}_LR_{lr}.csv')\n",
        "            labels_df.to_csv(out_file, index=False)\n",
        "            print(f\"Cluster labels saved to {out_file}\")\n",
        "\n",
        "            #Storing metrics record\n",
        "            results.append({'latent_dim': latent_dim,'n_clusters': n_clusters,'learning_rate': lr,'Silhouette': sil,'DBI': dbi})\n",
        "\n",
        "#Saving results\n",
        "results_df = pd.DataFrame(results)\n",
        "metrics_file = os.path.join(save_dir, 'DEC_grid_search_metrics.csv')\n",
        "results_df.to_csv(metrics_file, index=False)\n",
        "print(f\"\\nGrid search metrics saved to '{metrics_file}'\")"
      ],
      "metadata": {
        "id": "-Ap4Oz_Gw5yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VISUALISATION FOR BEST PERFORMING CLUSTERING METHODS**"
      ],
      "metadata": {
        "id": "LyNz5z30xjzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Autoencoder_KMeans_Clusters_4_Latent_10_LR_0.01.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "#Selecting feature columns\n",
        "#If you know other non-feature columns (like dates, IDs), you can add them here\n",
        "non_feature_cols = ['UserID', 'Cluster']\n",
        "\n",
        "feature_cols = [col for col in df.columns if col not in non_feature_cols]\n",
        "\n",
        "#Keeping only numeric features (safety)\n",
        "feature_cols = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "#Computing mean profile per cluster\n",
        "cluster_profile = df.groupby('Cluster')[feature_cols].mean()\n",
        "\n",
        "#Building radar chart\n",
        "categories = list(feature_cols)\n",
        "N = len(categories)\n",
        "\n",
        "#Angles for each axis\n",
        "angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
        "angles += angles[:1]  # close the loop\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "for cluster_id, row in cluster_profile.iterrows():\n",
        "    values = row.values.tolist()\n",
        "    values += values[:1]  # close the loop\n",
        "    plt.polar(angles, values, label=f'Cluster {cluster_id}', linewidth=2)\n",
        "\n",
        "#Axis labels\n",
        "plt.xticks(angles[:-1], categories, fontsize=8)\n",
        "\n",
        "#Moving title upwards using pad\n",
        "plt.title(\n",
        "    'Cluster Feature Profiles (Radar Chart)',\n",
        "    fontsize=14,\n",
        "    pad=30\n",
        ")\n",
        "\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.15))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JoSS1bgixmnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "#Loading the result file\n",
        "file_path = '/content/drive/MyDrive/Autoencoder_KMeans_Clusters_4_Latent_10_LR_0.01.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "#Selecting feature columns\n",
        "non_feature_cols = ['UserID', 'Cluster', 'AOV', 'AvgOrderDays']\n",
        "feature_cols = df.drop(columns=non_feature_cols, errors='ignore') \\\n",
        "                 .select_dtypes(include=['int64', 'float64']) \\\n",
        "                 .columns.tolist()\n",
        "\n",
        "#Computing mean profile per cluster\n",
        "cluster_profile = df.groupby('Cluster')[feature_cols].mean()\n",
        "\n",
        "#Radar chart setup\n",
        "categories = feature_cols\n",
        "N = len(categories)\n",
        "angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
        "angles += angles[:1]\n",
        "\n",
        "clusters = cluster_profile.index.tolist()\n",
        "n_clusters = len(clusters)\n",
        "\n",
        "plots_per_row = 2\n",
        "n_rows = math.ceil(n_clusters / plots_per_row)\n",
        "\n",
        "#Colour palette (one colour per cluster)\n",
        "colors = plt.cm.Set2(np.linspace(0, 1, n_clusters))\n",
        "\n",
        "Creating subplots\n",
        "fig, axes = plt.subplots(\n",
        "    n_rows,\n",
        "    plots_per_row,\n",
        "    figsize=(plots_per_row * 7, n_rows * 6),\n",
        "    subplot_kw=dict(polar=True)\n",
        ")\n",
        "\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, (cluster_id, row) in enumerate(cluster_profile.iterrows()):\n",
        "    values = row.values.tolist()\n",
        "    values += values[:1]\n",
        "\n",
        "    ax = axes[i]\n",
        "    ax.plot(angles, values, color=colors[i], linewidth=2)\n",
        "    ax.fill(angles, values, color=colors[i], alpha=0.25)\n",
        "\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(categories, fontsize=8)\n",
        "\n",
        "    # Move title upwards\n",
        "    ax.set_title(\n",
        "        f'Cluster {cluster_id}',\n",
        "        fontsize=14,\n",
        "        pad=25\n",
        "    )\n",
        "\n",
        "#Removing unused subplots\n",
        "for j in range(i + 1, len(axes)):\n",
        "    fig.delaxes(axes[j])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SxG9ljctx3jo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}